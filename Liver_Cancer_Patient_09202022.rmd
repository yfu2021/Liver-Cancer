---
title: "Liver Cancer Analysis"
Author: yfu2015
date: "2022-09-20"
geometry: "left=1.5cm,right=1.5cm,top=1.5cm,bottom=1.5cm"
output:
  pdf_document: 
    latex_engine: xelatex
  word_document: default
  html_document: default
---

# Executive Summary

By training 4 machine learning algorithms on Indian Liver Cancer data set (583 obs/11 variables), this analysis is to determine if liver cancer (represented by the target column named "dataset") can be identified by 10 available predictors/features. If yes, how accurate the model is and what are the top 5 predictors for each model respectively.

```{r}
##### Install packages ######

if(!require(tidyverse)) 
  install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(caret)) 
  install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(data.table)) 
  install.packages("data.table", repos = "http://cran.us.r-project.org")
if(!require(rpart)) 
  install.packages("rpart", repos = "http://cran.us.r-project.org")
if(!require(matrixStats)) 
  install.packages("matrixStats", repos = "http://cran.us.r-project.org")
if(!require(dslabs)) 
  install.packages("dslabs", repos = "http://cran.us.r-project.org")
#if(!require(genefilter)) install.packages("genefilter", repos = "http://cran.us.r-project.org")
if(!require(gam)) 
  install.packages("gam", repos = "http://cran.us.r-project.org")
if(!require(gridExtra)) 
  install.packages("gridExtra", repos = "http://cran.us.r-project.org")
if(!require(randomForest)) 
  install.packages("randomForest", repos = "http://cran.us.r-project.org")
if(!require(tinytex)) 
  install.packages("tinytex", repos = "http://cran.us.r-project.org")

##### loading packages #####

library(tidyverse)
library(caret)
library(data.table)
library(rpart)
library(matrixStats)
library(dslabs)
#library(genefilter)
library(gam)
library(gridExtra)
library(randomForest)
library(tinytex)

#tinytex::install_tinytex()
#update.packages(ask = FALSE, checkBuilt = TRUE)
#update.packages("tidyverse")
#update.packages()
#installed.packages()
#remove.packages(pkgs=row.names(x=installed.packages(priority="NA")))
#old.packages()
```

**Comment**

1. Download dataset indian-liver-patient-records directly from url <- "www.kaggle.com/uciml/indian-liver-patient-records"
Then push the dataset "indian-liver-patient-records” to github, finally, read it to R environment, and name it as "liver.df"
2. Albumin_and_Globulin_Ratio has 1% NA

```{r}
#liver.df <- read.csv("C:/Users/yfu/Desktop/LiverPatients_download.csv")
liver.df <- read.csv(url(
           "https://raw.githubusercontent.com/yfu2021/liver_cancer/master/LiverPatients_download.csv"
              )
            )
summary(liver.df)

```

**Comments**

1. Rename the data element 'Dataset' to 'Diagnosis', and change this data element type to factor with levels of 1 and 0 respectively, 1 is liver cancer, 0 is non liver cancer 
2. Clean the data - fill in the missing value of Albumin_and_Globulin_Ratio with median value of Albumin_and_Globulin_Ratio 
3. Remove the column 'Dataset' from the liver.df

```{r}
liver <- liver.df %>% 
         mutate(Diagnosis = factor(ifelse(Dataset==1, 1, 0)),
                Gender = as.numeric(ifelse(Gender=="Female", 0, 1)),
                Albumin_and_Globulin_Ratio = ifelse(is.na(Albumin_and_Globulin_Ratio),
                                        median(Albumin_and_Globulin_Ratio,na.rm=TRUE),
                                        Albumin_and_Globulin_Ratio) # Replace NA with median value
              ) %>% select(-Dataset)
```

**Comment** 

1. Check the distribution of columns/predictors.
2. Check if there are any data elements with very few non-unique values or close to zero variation

```{r}

str(liver)

sum(liver$Diagnosis==1)
sum(liver$Diagnosis==0)

All_Patients_Age <- liver$Age
hist(All_Patients_Age, main="Histogram of Age")   
Cancer <- liver %>% filter(Diagnosis==1)
Liver_Cancer_Patients_Age <- Cancer$Age
hist(Liver_Cancer_Patients_Age, main="Histogram of Age for Cancer Patient Only")


#Distribution of all predictors vs target value

p1 <- liver %>% ggplot(aes(Diagnosis, Age)) + 
                geom_jitter(width = 0.1, alpha = 0.2)
p2 <- liver %>% ggplot(aes(Diagnosis,Gender)) + 
                geom_jitter(width = 0.1, alpha = 0.2)
p3 <- liver %>% ggplot(aes(Diagnosis,Total_Bilirubin)) + 
                geom_jitter(width = 0.1, alpha = 0.2) 
p4 <- liver %>% ggplot(aes(Diagnosis,Direct_Bilirubin)) + 
                geom_jitter(width = 0.1, alpha = 0.2) 
p5 <- liver %>% ggplot(aes(Diagnosis,Alkaline_Phosphotase)) + 
                geom_jitter(width = 0.1, alpha = 0.2) 
p6 <- liver %>% ggplot(aes(Diagnosis,Alamine_Aminotransferase)) + 
                geom_jitter(width = 0.1, alpha = 0.2) 
p7 <- liver %>% ggplot(aes(Diagnosis,Aspartate_Aminotransferase)) + 
                geom_jitter(width = 0.1, alpha = 0.2)
p8 <- liver %>% ggplot(aes(Diagnosis,Total_Protiens)) + 
                geom_jitter(width = 0.1, alpha = 0.2)
p9 <- liver %>% ggplot(aes(Diagnosis,Albumin)) + 
                geom_jitter(width = 0.1, alpha = 0.2) 
p10 <- liver %>% ggplot(aes(Diagnosis,Albumin_and_Globulin_Ratio)) + 
                geom_jitter(width = 0.1, alpha = 0.2)


grid.arrange(p1, p2,p3,p4,p5,p6,p7,p8,p9,p10, nrow=2, ncol = 5)


#Check if there is any data element with very few non-unique values or close to zero variation

nearZeroVar(liver)

```

**Finding**

None of a data element has a few non-unique values and close to zero variation 

**Comment**

Correlation analysis is performed on the target value and other 10 predictors on the original data set liver.df, to see if there are correlation exist between any of the predictors and the target value/column. 

```{r}
liver.df <- liver.df %>% mutate(Gender=as.numeric( ifelse(Gender=="Female", 0, 1) ))

cor(liver.df, use="pairwise.complete")
```

**Finding**

1. The target value (named "Dataset" in liver.df) is not independent with all predictors, e.g, the target value 'Dataset' has 24.6% of coefficient with Direct_Bilirubin...
2. No further t-testing will be done on predictors to extract a subset for ML models training in order to improve accuracy based on a certain threshold of p-value.

**Comment**

1. From the summary statistic analysis of data set "liver.df, it is observed that the unit of each column is different, there exists big variance of the values among all predictors.
2. Will center and scale the columns of the predictors of the data set of 'liver'.
3. set.seed(1, sample.kind = "Rounding") # simulate R 3.5, there is warning message comes out. This is not a warning or a cause for alarm - it’s a confirmation that R is using the alternate seed generation method, and it should expect #to receive this message in your console.

```{r}
# centering and scaling on all predictors

options(digits = 3)

#set.seed(1) # if using R 3.5 or earlier
set.seed(1, sample.kind = "Rounding") # if using R 3.6 or later


# setp 1 - center and scale 10 predictors

Predictors <- liver %>%  select(-Diagnosis)
Diagnosis <- liver$Diagnosis  ###%>%  select(Diagnosis)  

x_centered <- sweep(Predictors, 2, colMeans(Predictors))
x_scaled <- sweep(x_centered, 2, colSds(as.matrix(Predictors)), FUN = "/")
```

**Comment**

1. Next, correlation analysis will be performed on the scaled data set 'x_scaled'.
2. Virtually present the correlation of the predictors/features of the scaled data set 'x_scaled'

```{r}
cor(x_scaled, use="pairwise.complete") ##%>% knitr::kable()
image(as.matrix(cor(x_scaled, use="pairwise.complete")), axes = TRUE, 
      main = "Correlation of All Predictors")  
```

**Observation**

1. Total_Bilirubin and Direct_Bilirubin is highly correlated (correlated coefficient 0.8746); 
2. Alamine_Aminotransferase and Aspartate_Aminotransferase is highly correlated (correlated coefficient 0.7920); 
3. Total_Protiens is highly correlated with |Albumin (correlated coefficient 0.784053)
4. Albumin_and_Globulin_Ratio is highly correlated with Albumin (correlated coefficient  0.6861)

```{r}
# principal component analysis

pca <- prcomp(x_scaled)
summary(pca) 
```

**Observations**

1. PC1 & PC2 cumulative proportion only accounts for 48% of the variance
2. 6 principal components are needed to explain about 89% of the variance.
3. 7 principal components are needed to explain about 95% of the variance.
4. 8 principal components are needed to explain about 98% of the variance.

```{r}
# Some basic analysis on principal component analysis 

# 1. By looking at the distribution of PC1 and PC2, Liver cancer patients 
#    tend to have larger values of PC2 than non-liver cancer patient

data.frame(pc_1 = pca$x[,1], pc_2 = pca$x[,2], 
           type=Diagnosis ) %>%
  ggplot(aes(pc_1, pc_2, color = type)) +
  geom_point()
```

```{r}
#2. Liver cancer patients tend to have larger variance of PC3 
#   than non-liver cancer patient.

data.frame(pc_3 = pca$x[,3], pc_4 = pca$x[,4], 
           type=Diagnosis ) %>%
  ggplot(aes(pc_3, pc_4, color = type)) +
  geom_point()
```

```{r}
#3. Distribution of IQRs from PC 1 through PC 10

data.frame(type = Diagnosis, pca$x[,1:10]) %>%        
  gather(key = "PC", value = "value", -type) %>%
  ggplot(aes(PC, value, fill = type)) +
  geom_boxplot()
```

**Findings**

1. All IQRs of PC1 - PC10 overlapped categorized by the liver cancer (1, 0)
2. PC1 has biggest difference of IQR categorized by the liver cancer (1, 0), but not significant enough to predict the liver cancer. 
3. All features/predictors will be included for 4 training ML models

```{r}
#Split the Scaled data to train set and test set

set.seed(1, sample.kind="Rounding") # if using R 3.5 or earlier, use `set.seed(1)`
test_index <- createDataPartition(Diagnosis, times = 1, p = 0.1, list = FALSE)
test_x <- x_scaled[test_index,]   ## str(test_x)
test_y <- Diagnosis[test_index]    ## length(test_y)

train_x <- x_scaled[-test_index,]   ## str(train_x)
train_y <- Diagnosis[-test_index]   ## length(train_y)
```

```{r setup, warning=FALSE}
#Logistic regression model   

# set.seed(1) if using R 3.5 or earlier
set.seed(1, sample.kind = "Rounding") # if using R 3.6 or later
train_glm <- train(train_x, train_y, method = "glm")
knitr::opts_chunk$set(warning = FALSE, message = FALSE) 
glm_pred <- predict(train_glm, test_x)
logistic_Accuracy2 <-  mean(glm_pred == test_y)

glm_sensitivity <- rbind(sensitivity(glm_pred, test_y), specificity(glm_pred, test_y))
rownames( glm_sensitivity )  <-  c("sensitivity","specificity") 
colnames( glm_sensitivity )  <-  c("Logistic Regression") 

# Top 5 important Predictors

a <- varImp(train_glm)[[1]]
varImp_df_glm <- data.frame(matrix(c(rownames(a), as.numeric(a[,1])), nrow=10, ncol=2, 
                                   dimnames=list(c(seq(1:10)), c("Predictor", "Value"))))
varImp_df_glm <- varImp_df_glm %>% mutate(Value=as.numeric(Value)) %>% arrange(desc(Value)) 
rownames(varImp_df_glm) <- seq(1:10)
Top_5_Glm_Predictors <- varImp_df_glm[1:5,1]   
Top_5_GLM_predictors <- data.frame(Top_5_Glm_Predictors)
```

```{r}
#K-nearest neighbors model K-nearest neighbors model    

# set.seed(1)
set.seed(1, sample.kind = "Rounding") # simulate R 3.5
control <- trainControl(method = "cv", number = 10, p = .9)
train_knn <- train(train_x, train_y, method = "knn", 
                   tuneGrid = data.frame(k = seq(9, 71, 2)),
                   trControl = control)
ggplot(train_knn, highlight=TRUE, main="k nearst neighbor Model accuracy vs k")
train_knn$bestTune
knn_pred <- predict( train_knn, test_x)
K_nearst_Accuracy2 <- mean(knn_pred== test_y)

knn_sensitivity <- rbind(sensitivity(knn_pred, test_y), specificity(knn_pred, test_y))
rownames( knn_sensitivity )  <-  c("sensitivity","specificity") 
colnames( knn_sensitivity )  <-  c("K_Nearst_Neighbor") 

# Top 5 important Predictors
a <- varImp(train_knn)[[1]]
varImp_df_knn <- data.frame(matrix(c(rownames(a), as.numeric(a[,1])), nrow=10, ncol=2, 
                                   dimnames=list(c(seq(1:10)), c("Predictor", "Value"))))
varImp_df_knn <- varImp_df_knn %>% mutate(Value=as.numeric(Value)) %>% arrange(desc(Value)) 
rownames(varImp_df_knn) <- seq(1:10)
Top_5_knn_Predictors <- varImp_df_knn[1:5,1]   
Top_5_knn_predictors <- data.frame(Top_5_knn_Predictors)
```

```{r}
#random Forest model
                  
# set.seed(1)
set.seed(1, sample.kind = "Rounding") # simulate R 3.5   

tuning <- data.frame(mtry = c(1,2,3,4,5,6,7,8))
train_rf <- train(train_x, train_y,
                  method = "rf",
                  tuneGrid = tuning,
                  importance = TRUE)

ggplot(train_rf,highlight=TRUE, title="random Forest Model Acuracy distribution")

rf_pred <- predict(train_rf, test_x)
rf_Accuracy2 <- mean(rf_pred == test_y)

rf_sensitivity <- rbind(sensitivity(rf_pred, test_y), specificity(rf_pred, test_y))
rownames( rf_sensitivity )  <-  c("sensitivity","specificity") 
colnames( rf_sensitivity )  <-  c("random Forest") 

# Top 5 important Predictors
a <- varImp(train_rf)[[1]]  
varImp_df_rf <- data.frame(matrix(c(rownames(a), as.numeric(a[,1])), nrow=10, ncol=2, 
                                  dimnames=list(c(seq(1:10)), c("Predictor", "Value"))))
varImp_df_rf <- varImp_df_knn %>% mutate(Value=as.numeric(Value)) %>% arrange(desc(Value)) 
rownames(varImp_df_rf) <- seq(1:10)
Top_5_rf_Predictors <- varImp_df_rf[1:5,1]   
Top_5_rf_predictors <- data.frame(Top_5_rf_Predictors)
```


```{r}
#Local Polynomial Regression Model

# set.seed(1)
set.seed(1, sample.kind = "Rounding") # simulate R 3.5
grid <- expand.grid(span = seq(0.15, 0.65, len = 10), degree = 1)
train_loess <- train(train_x, train_y, method = "gamLoess", tuneGrid=grid)
knitr::opts_chunk$set(warning = FALSE, message = FALSE) 
ggplot(train_loess, highlight=TRUE)
loess_pred <- predict(train_loess, test_x)
Loess_Accuracy2 <- mean(loess_pred == test_y)   

Loess_sensitivity <- rbind(sensitivity(loess_pred, test_y), specificity(loess_pred, test_y))
rownames( Loess_sensitivity )  <-  c("sensitivity","specificity") 
colnames( Loess_sensitivity )  <-  c("Local Polynomial Regression") 

#### Top 5 important Predictors ####
a <- varImp(train_loess)[[1]]
varImp_df_loess <- data.frame(matrix(c(rownames(a), as.numeric(a[,1])), nrow=10, ncol=2, 
                                     dimnames=list(c(seq(1:10)), c("Predictor", "Value"))))
varImp_df_loess <- varImp_df_loess %>% mutate(Value=as.numeric(Value)) %>% arrange(desc(Value)) 
rownames(varImp_df_loess) <- seq(1:10)
Top_5_Loess_Predictors <- varImp_df_loess[1:5,1]   
Top_5_Loess_predictors <- data.frame(Top_5_Loess_Predictors)
```

```{r}
#Format/combine the results from all ML models

Accuracy_results <- data_frame(Method = "Logistic Regression Model", Accuracy = logistic_Accuracy2)
Accuracy_results <- bind_rows(Accuracy_results,
                              data_frame(Method="K nearst neighbors Model",
                                         Accuracy = K_nearst_Accuracy2 ))
Accuracy_results <- bind_rows(Accuracy_results,
                              data_frame(Method="Random Forest",  
                                         Accuracy =  rf_Accuracy2 ))
Accuracy_results <- bind_rows(Accuracy_results,
                              data_frame(Method="Local Polynomial Regression Model",  
                                         Accuracy =  Loess_Accuracy2 ))
Accuracy_results %>% knitr::kable(align='c')
```

```{r}
#Format Sensitivity and Specificity Results of all Models

cbind(glm_sensitivity , knn_sensitivity, rf_sensitivity, Loess_sensitivity) %>% 
      knitr::kable(align='c')
```

```{r}
#Format top 5 important predictors/features 

cbind(Top_5_GLM_predictors, Top_5_knn_predictors, Top_5_rf_predictors, Top_5_Loess_predictors)%>% 
      knitr::kable()
```
# Conclusion

1. The prediction on the test data by applying 4 machine learning models tend to have lower sensitivity but high specificity, which means the models tend to predict accurately on the true negative liver cancer patients, but not the true positive liver cancer patients
2. Currently, the data set is pretty small which only contains 583 instances. Larger data set is encouraged to be collected for training ML models. Giver the reason that in Indian, only around 3-5 instances of liver cancer per 100,000 persons which means the prevalence of liver cancer is very low in India. Larger data might can overcome the possible imbalanced small data sets. 
3. It is also encouraged to collect other useful/critical information such as patient family cancer history, patient family liver cancer history; patient Hepatitis B/C history; alcohol intake history; weight change information, nutrition habit, etc. for comprehensive analysis on the liver cancer study.

# Reference

1. [HarvardX PH125.8x Data Science: Machine Learning -- 7.1 Final Assessment: Breast Cancer Prediction Project (Verified Learners only)](https://learning.edx.org/course/course-v1:HarvardX+PH125.8x+2T2021/block-v1:HarvardX+PH125.8x+2T2021+type@sequential+block@5793085a5b9e471fbfcb47caf6ffe197/block-v1:HarvardX+PH125.8x+2T2021+type@vertical+block@2c2081ed0e7d41d7bb5649f7dca6f2da)
2. [Liver Cancer 101: What are the Five Gospel Truths about the Disease](https://www.medicahospitals.in/blog/liver-cancer-101-what-are-the-five-gospel-truths-about-the-disease/)
